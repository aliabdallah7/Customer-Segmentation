# Plotting dendrogram
plot(Hierar_cl)
# Choosing no. of clusters
# Cutting tree by height
abline(h =2 , col = "red")
# Cutting tree by no. of clusters
fit <- cutree(Hierar_cl, k = 10 )
fit
table(fit)
rect.hclust(Hierar_cl, k = 10, border = "green")
# Hierarchical clustering
hc2 <- agnes(dataset[,2:5], method = "average")
# Plot the obtained dendrogram
pltree(hc2, main = "Dendrogram of agnes")
# Load libraries
library(cluster)
library(dbscan)
library(factoextra)
library(fpc)
library(NbClust)
library(plotrix)
library(purrr)
library(tidyverse)
# Read the dataset
dataset <- read.csv("CustomerSegmentation.csv")
# Check for missing values
any(is.na(dataset))
# Display dataset information
str(dataset)
names(dataset)  # Column names
# Rename the columns
names(dataset) <- c("ID", "Gender", "Age", "Annual_Income", "Spending_Score")
head(dataset)
# Load libraries
library(cluster)
library(dbscan)
library(factoextra)
library(fpc)
library(NbClust)
library(plotrix)
library(purrr)
library(tidyverse)
# Read the dataset
dataset <- read.csv("CustomerSegmentation.csv")
# Check for missing values
any(is.na(dataset))
# Remove rows with missing values
dataset <- na.omit(dataset)
# Display dataset information
str(dataset)
names(dataset)  # Column names
# Rename the columns
names(dataset) <- c("ID", "Gender", "Age", "Annual_Income", "Spending_Score")
head(dataset)
# Load libraries
library(cluster)
library(dbscan)
library(factoextra)
library(fpc)
library(NbClust)
library(plotrix)
library(purrr)
library(tidyverse)
# Read the dataset
dataset <- read.csv("CustomerSegmentation.csv")
# Check for missing values
any(is.na(dataset))
# Remove rows with missing values
dataset <- na.omit(dataset)
# Remove rows with missing values
dataset <- dataset[complete.cases(dataset), ]
# Remove rows with negative values
dataset <- dataset %>%
filter_all(all_vars(. >= 0))
# Display dataset information
str(dataset)
names(dataset)  # Column names
# Rename the columns
names(dataset) <- c("ID", "Gender", "Age", "Annual_Income", "Spending_Score")
head(dataset)
any(dataset < 0)
# Load libraries
library(cluster)
library(dbscan)
library(factoextra)
library(fpc)
library(NbClust)
library(plotrix)
library(purrr)
library(tidyverse)
# Read the dataset
dataset <- read.csv("CustomerSegmentation.csv")
# Check for missing values
any(is.na(dataset))
# Remove rows with missing values
dataset <- na.omit(dataset)
# Remove rows with missing values
dataset <- dataset[complete.cases(dataset), ]
# Remove rows with negative values
dataset <- dataset %>%
filter_all(all_vars(. >= 0))
# Ensure there isn't negative values any more
any(dataset < 0)
# Display dataset information
str(dataset)
names(dataset)  # Column names
# Rename the columns
names(dataset) <- c("ID", "Gender", "Age", "Annual_Income", "Spending_Score")
head(dataset)
# Summary statistics and standard deviations
summary_and_sd <- function(column) {
summary_col <- summary(column)
sd_col <- sd(column)
cat("Summary:", summary_col, "\n")
cat("Standard Deviation:", sd_col, "\n")
}
summary(dataset$Age)
sd(dataset$Age) # Standard Deviation
summary_and_sd(dataset$Annual_Income)
summary_and_sd(dataset$Spending_Score)
# Function to create barplot and pie chart for gender
visualize_gender <- function(gender_column) {
a <- table(gender_column)
barplot(a, main="BarPlot to display Gender Comparison",
ylab="Count", xlab="Gender", col=rainbow(2), legend=rownames(a))
pct <- round(a/sum(a)*100)
lbs <- paste(c("Female","Male")," ", pct, "%")
pie3D(a, labels=lbs, main="Pie Chart Depicting Ratio of Female and Male")
}
# Visualize gender
visualize_gender(dataset$Gender)
# Function to create histogram and boxplot
visualize_numeric <- function(column, color, title, xlab, ylab) {
hist(column, col=color, main=title, xlab=xlab, ylab=ylab, labels=TRUE)
boxplot(column, col=color, main=paste("Boxplot for Descriptive Analysis of", xlab))
}
# Visualize age distribution
visualize_numeric(dataset$Age, "yellow", "Histogram and Boxplot for Age", "Age Class", "Frequency")
# Analysis & Visualization of the Annual Income of the Customers
boxplot(dataset$Annual_Income,
horizontal=TRUE,
col="pink",
main="BoxPlot for Descriptive Analysis of Annual Income")
hist(dataset$Annual_Income,
col="red",
main="Histogram for Annual Income",
xlab="Annual Income Class",
ylab="Frequency",
labels=TRUE)
plot(density(dataset$Annual_Income),
col="blue",
main="Density Plot for Annual Income",
xlab="Annual Income Class",
ylab="Density")
polygon(density(dataset$Annual_Income),
col="blue")
# Analysis & Visualization of Spending Score of the customer
boxplot(dataset$Spending_Score,
horizontal=TRUE,
col="blue",
main="BoxPlot for Descriptive Analysis of Spending Score")
hist(dataset$Spending_Score,
main="HistoGram for Spending Score",
xlab="Spending Score Class",
ylab="Frequency",
col="blue",
labels=TRUE)
plot(density(dataset$Spending_Score),
col="blue",
main="Density Plot for Spending Score",
xlab="Spending Score Class",
ylab="Density")
polygon(density(dataset$Spending_Score),
col="blue")
# Encoding categorical data
dataset$Gender <- ifelse(dataset$Gender == "Male",1,0)
head(dataset)
# scaling the dataset
dataset[,2:5] = scale(dataset[,2:5])
head(dataset)
# K-means Algorithm
# Determine and visualize the optimal number of clusters
# Elbow method
fviz_nbclust(dataset[,2:5], kmeans, method = "wss") +
geom_vline(xintercept = 8, linetype = 2) +
labs(subtitle = "Elbow method")
# Silhouette Method
fviz_nbclust(dataset[,2:5], kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
# Gap Statistic Method
set.seed(125)
fviz_nbclust(dataset[,2:5], kmeans, method = "gap_stat") +
labs(subtitle = "Gap statistic method")
# According to these observations, itâ€™s possible to define k = 9 as the optimal number of clusters in the data.
k9<-kmeans(dataset[,2:5],9,iter.max=100,nstart=50,algorithm="Lloyd")
k9
# K-means clusters
clusters <- k9$cluster
bl <- as.character(clusters)
# Create a plot of the customers segments
fviz_cluster(k9, dataset[,2:5], geom = c("point"),ellipse.type = "euclid")
# function for colors
kCols = function(vec){cols=rainbow (length (unique (vec)))
return (cols[as.numeric(as.factor(vec))])}
# Plotting K-means
plot(dataset[,4:5], col = kCols(clusters), pch =19, xlab = "K-means", ylab = "classes")
legend("bottomleft",unique(bl),fill = unique(kCols(clusters)))
# Visualizing the Clustering Results using the First Two Principle Components
pc_clusters = prcomp(dataset[,2:5],scale=FALSE) #PCA
summary(pc_clusters)
pc_clusters$rotation[,1:2]
# Plotting K-means based on results from the cluster analysis and PCA
plot(pc_clusters$x[,1:2], col = kCols(clusters),pch =19,xlab = "K-means",ylab = "classes")
legend("bottomleft",unique(bl),fill = unique(kCols(clusters)))
# Dbscan Mtoh
# Estimate Epsilon (eps) with kNNdistplot
eps_plot = kNNdistplot(dataset[,2:5], k = 3)
eps_plot %>% abline(h = 0.58, lty = 2)
# DBSCAN Clustering
set.seed(7)
dbscan_clusters <- dbscan(dataset[,2:5],eps = 0.58 , minPts = 4)
# Display DBSCAN Cluster Results
dbscan_clusters
# Visualize DBSCAN Clusters
fviz_cluster(dbscan_clusters, dataset[,2:5], geom = "point")
# K-medoids
# Estimating the optimal number of clusters
# Silhouette Method
fviz_nbclust(dataset[,2:5], pam, method = "silhouette") +
labs(subtitle = "Silhouette method")
# computes PAM algorithm with k = 10
pam.res <- pam(dataset[,2:5], 10)
print(pam.res)
# Cluster medoids
pam.res$medoids
# Cluster numbers
head(pam.res$clustering)
# If we want to add the point classifications to the original data, use this
dd <- cbind(dataset[,2:5], cluster = pam.res$cluster)
head(dd)
# Visualizing PAM clusters
fviz_cluster(
pam.res,
data = dataset[,2:5],
stand = TRUE,
axes = c(1, 2),
geom = "point",
show.clust.cent = TRUE,
ellipse = TRUE,
ellipse.type = "euclid",
ellipse.level = 0.95,
ellipse.alpha = 0.2,
pointsize = 1.5,
labelsize = 12,
main = "Cluster plot",
ggtheme = theme_grey()
)
# hierarchical_Clustering Algorithm
# Finding distance matrix
# Estimating the optimal number of clusters
fviz_nbclust(dataset[,2:5], hcut, method = "silhouette") +
labs(subtitle = "Silhouette method")
distance_mat <- dist(dataset[,2:5], method = 'euclidean')
distance_mat
# Fitting Hierarchical clustering Model
# to training dataset
set.seed(240)  # Setting seed
Hierar_cl <- hclust(distance_mat, method = "average")
Hierar_cl
# Plotting dendrogram
plot(Hierar_cl)
# Choosing no. of clusters
# Cutting tree by height
abline(h =2 , col = "red")
# Cutting tree by no. of clusters
fit <- cutree(Hierar_cl, k = 10 )
fit
table(fit)
rect.hclust(Hierar_cl, k = 10, border = "green")
# Hierarchical clustering
hc2 <- agnes(dataset[,2:5], method = "average")
# Plot the obtained dendrogram
pltree(hc2, main = "Dendrogram of agnes")
eps_plot = kNNdistplot(dataset[,2:5], k = 3)
eps_plot %>% abline(h = 0.58, lty = 2)
set.seed(7)
dbscan_clusters <- dbscan(dataset[,2:5],eps = 0.58 , minPts = 3)
dbscan_clusters <- dbscan(dataset[,2:5],eps = 0.58)
dbscan_clusters
dbscan_clusters <- dbscan(dataset[,2:5],eps = 0.58 , minPts = 5)
# Dbscan Mtoh
# Estimate Epsilon (eps) with kNNdistplot
eps_plot = kNNdistplot(dataset[,2:5], k = 3)
eps_plot %>% abline(h = 0.58, lty = 2)
# DBSCAN Clustering
set.seed(7)
dbscan_clusters <- dbscan(dataset[,2:5],eps = 0.58 , MinPts = 5)
# Display DBSCAN Cluster Results
dbscan_clusters
# Dbscan Mtoh
# Estimate Epsilon (eps) with kNNdistplot
eps_plot = kNNdistplot(dataset[,2:5], k = 3)
eps_plot %>% abline(h = 0.58, lty = 2)
# DBSCAN Clustering
set.seed(7)
dbscan_clusters <- dbscan(dataset[,2:5],eps = 0.58 , MinPts = 4)
# Display DBSCAN Cluster Results
dbscan_clusters
# Visualize DBSCAN Clusters
fviz_cluster(dbscan_clusters, dataset[,2:5], geom = "point")
# Load libraries
library(cluster)
library(dbscan)
library(factoextra)
library(fpc)
library(NbClust)
library(plotrix)
library(purrr)
library(tidyverse)
# Read the dataset
dataset <- read.csv("CustomerSegmentation.csv")
# Check for missing values
any(is.na(dataset))
# Remove rows with missing values
dataset <- na.omit(dataset)
# Remove rows with missing values
dataset <- dataset[complete.cases(dataset), ]
# Remove rows with negative values
dataset <- dataset %>%
filter_all(all_vars(. >= 0))
# Ensure there isn't negative values any more
any(dataset < 0)
# Display dataset information
str(dataset)
names(dataset)  # Column names
# Rename the columns
names(dataset) <- c("ID", "Gender", "Age", "Annual_Income", "Spending_Score")
head(dataset)
# Summary statistics and standard deviations
summary_and_sd <- function(column) {
summary_col <- summary(column)
sd_col <- sd(column)
cat("Summary:", summary_col, "\n")
cat("Standard Deviation:", sd_col, "\n")
}
summary(dataset$Age)
sd(dataset$Age) # Standard Deviation
summary_and_sd(dataset$Annual_Income)
summary_and_sd(dataset$Spending_Score)
# Function to create barplot and pie chart for gender
visualize_gender <- function(gender_column) {
a <- table(gender_column)
barplot(a, main="BarPlot to display Gender Comparison",
ylab="Count", xlab="Gender", col=rainbow(2), legend=rownames(a))
pct <- round(a/sum(a)*100)
lbs <- paste(c("Female","Male")," ", pct, "%")
pie3D(a, labels=lbs, main="Pie Chart Depicting Ratio of Female and Male")
}
# Visualize gender
visualize_gender(dataset$Gender)
# Function to create histogram and boxplot
visualize_numeric <- function(column, color, title, xlab, ylab) {
hist(column, col=color, main=title, xlab=xlab, ylab=ylab, labels=TRUE)
boxplot(column, col=color, main=paste("Boxplot for Descriptive Analysis of", xlab))
}
# Visualize age distribution
visualize_numeric(dataset$Age, "yellow", "Histogram and Boxplot for Age", "Age Class", "Frequency")
# Analysis & Visualization of the Annual Income of the Customers
boxplot(dataset$Annual_Income,
horizontal=TRUE,
col="pink",
main="BoxPlot for Descriptive Analysis of Annual Income")
hist(dataset$Annual_Income,
col="red",
main="Histogram for Annual Income",
xlab="Annual Income Class",
ylab="Frequency",
labels=TRUE)
plot(density(dataset$Annual_Income),
col="blue",
main="Density Plot for Annual Income",
xlab="Annual Income Class",
ylab="Density")
polygon(density(dataset$Annual_Income),
col="blue")
# Analysis & Visualization of Spending Score of the customer
boxplot(dataset$Spending_Score,
horizontal=TRUE,
col="blue",
main="BoxPlot for Descriptive Analysis of Spending Score")
hist(dataset$Spending_Score,
main="HistoGram for Spending Score",
xlab="Spending Score Class",
ylab="Frequency",
col="blue",
labels=TRUE)
plot(density(dataset$Spending_Score),
col="blue",
main="Density Plot for Spending Score",
xlab="Spending Score Class",
ylab="Density")
polygon(density(dataset$Spending_Score),
col="blue")
# Encoding categorical data
dataset$Gender <- ifelse(dataset$Gender == "Male",1,0)
head(dataset)
# scaling the dataset
dataset[,2:5] = scale(dataset[,2:5])
head(dataset)
# K-means Algorithm
# Determine and visualize the optimal number of clusters
# Elbow method
fviz_nbclust(dataset[,2:5], kmeans, method = "wss") +
geom_vline(xintercept = 8, linetype = 2) +
labs(subtitle = "Elbow method")
# Silhouette Method
fviz_nbclust(dataset[,2:5], kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
# Gap Statistic Method
set.seed(125)
fviz_nbclust(dataset[,2:5], kmeans, method = "gap_stat") +
labs(subtitle = "Gap statistic method")
# According to these observations, itâ€™s possible to define k = 9 as the optimal number of clusters in the data.
k9<-kmeans(dataset[,2:5],9,iter.max=100,nstart=50,algorithm="Lloyd")
k9
# K-means clusters
clusters <- k9$cluster
bl <- as.character(clusters)
# Create a plot of the customers segments
fviz_cluster(k9, dataset[,2:5], geom = c("point"),ellipse.type = "euclid")
# function for colors
kCols = function(vec){cols=rainbow (length (unique (vec)))
return (cols[as.numeric(as.factor(vec))])}
# Plotting K-means
plot(dataset[,4:5], col = kCols(clusters), pch =19, xlab = "K-means", ylab = "classes")
legend("bottomleft",unique(bl),fill = unique(kCols(clusters)))
# Visualizing the Clustering Results using the First Two Principle Components
pc_clusters = prcomp(dataset[,2:5],scale=FALSE) #PCA
summary(pc_clusters)
pc_clusters$rotation[,1:2]
# Plotting K-means based on results from the cluster analysis and PCA
plot(pc_clusters$x[,1:2], col = kCols(clusters),pch =19,xlab = "K-means",ylab = "classes")
legend("bottomleft",unique(bl),fill = unique(kCols(clusters)))
# Dbscan Mtoh
# Estimate Epsilon (eps) with kNNdistplot
eps_plot = kNNdistplot(dataset[,2:5], k = 3)
eps_plot %>% abline(h = 0.58, lty = 2)
# DBSCAN Clustering
set.seed(7)
dbscan_clusters <- dbscan(dataset[,2:5],eps = 0.58 , MinPts = 4)
# Display DBSCAN Cluster Results
dbscan_clusters
# Visualize DBSCAN Clusters
fviz_cluster(dbscan_clusters, dataset[,2:5], geom = "point")
# K-medoids
# Estimating the optimal number of clusters
# Silhouette Method
fviz_nbclust(dataset[,2:5], pam, method = "silhouette") +
labs(subtitle = "Silhouette method")
# computes PAM algorithm with k = 10
pam.res <- pam(dataset[,2:5], 10)
print(pam.res)
# Cluster medoids
pam.res$medoids
# Cluster numbers
head(pam.res$clustering)
# If we want to add the point classifications to the original data, use this
dd <- cbind(dataset[,2:5], cluster = pam.res$cluster)
head(dd)
# Visualizing PAM clusters
fviz_cluster(
pam.res,
data = dataset[,2:5],
stand = TRUE,
axes = c(1, 2),
geom = "point",
show.clust.cent = TRUE,
ellipse = TRUE,
ellipse.type = "euclid",
ellipse.level = 0.95,
ellipse.alpha = 0.2,
pointsize = 1.5,
labelsize = 12,
main = "Cluster plot",
ggtheme = theme_grey()
)
# hierarchical_Clustering Algorithm
# Finding distance matrix
# Estimating the optimal number of clusters
fviz_nbclust(dataset[,2:5], hcut, method = "silhouette") +
labs(subtitle = "Silhouette method")
distance_mat <- dist(dataset[,2:5], method = 'euclidean')
distance_mat
# Fitting Hierarchical clustering Model
# to training dataset
set.seed(240)  # Setting seed
Hierar_cl <- hclust(distance_mat, method = "average")
Hierar_cl
# Plotting dendrogram
plot(Hierar_cl)
# Choosing no. of clusters
# Cutting tree by height
abline(h =2 , col = "red")
# Cutting tree by no. of clusters
fit <- cutree(Hierar_cl, k = 10 )
fit
table(fit)
rect.hclust(Hierar_cl, k = 10, border = "green")
# Hierarchical clustering
hc2 <- agnes(dataset[,2:5], method = "average")
# Plot the obtained dendrogram
pltree(hc2, main = "Dendrogram of agnes")
